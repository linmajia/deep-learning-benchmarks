WorkDir=.
ModelDir=$WorkDir$/Output/$ConfigName$
DataDir=/home/dl/data/cntk
precision=float
stderr  = "./output_$ConfigName$_Train.log"

deviceId=Auto
minibatchSize=1024
epochSize=4096
maxEpochs=10

makeMode=false

command=Train

featureDim = 512 
labelDim = 1000 
hiddenDim = 2048

initOnCPUOnly=true
parallelTrain=false
prefetch=true
traceLevel=1

Train=[
    action=train
    modelPath=$ModelDir$/fc26752l6
    deviceId=$deviceId$
    traceLevel=1

    SimpleNetworkBuilder=[
        #layerSizes=$featureDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$labelDim$
        layerSizes=$featureDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$hiddenDim$:$labelDim$
        initOnCPUOnly=true
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        applyMeanVarNorm=false
        initValueScale=1.0
        uniformInit=true
        needPrior=false
    ]
    
    SGD=[
        epochSize=$epochSize$
        minibatchSize=$minibatchSize$
        maxEpochs=$maxEpochs$
        learningRatesPerMB=0.01
        perfTraceLevel = 1
        numMBsToShowResult=1
        momentumPerSample=0
        dropoutRate=0.0

        #epochSize=4096
        #minibatchSize=256
        #maxEpochs=2
        #learningRatesPerMB=0.01
        #numMBsToShowResult=4
        #momentumPerSample=0
        #dropoutRate=0.0
        
        ParallelTrain=[
            parallelizationMethod=DataParallelSGD
            distributedMBReading=true
            parallelizationStartEpoch=1
            DataParallelSGD=[
                gradientBits=1
            ]
        ]

        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]

reader=[
    readerType=UCIFastReader
    file=$DataDir$/data26752.txt
    features=[
        dim=$featureDim$
        start=1
    ]
    labels=[
        dim=1
        start=0
        labelDim=$labelDim$
        labelMappingFile=$DataDir$/labelmap26752.txt
        #labelMappingFile=$WorkDir$/labelmap.txt
    ]
]
